{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1kw5zR_qqfK__3-0MPdTQ0DSFQqZ5bO8K","timestamp":1761245186258}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# WELCOME\n","\n","This notebook will guide you through two increasingly significant applications in the realm of Generative AI: RAG (Retrieval Augmented Generation) chatbots and text summarization for big text.\n","\n","Through two distinct projects, you will explore these technologies and enhance your skills. Detailed descriptions of the projects are provided below."],"metadata":{"id":"DxOaSxtJWV1G"}},{"cell_type":"markdown","source":["## Project 1: Building a Chatbot with a PDF Document (RAG)\n","\n","In this project, you will develop a chatbot using a provided PDF document from web page. You will utilize the Langchain framework along with a large language model (LLM) such as GPT or Gemini. The chatbot will leverage the Retrieval Augmented Generation (RAG) technique to comprehend the document's content and respond to user queries effectively.\n","\n","### **Project Steps:**\n","\n","- **1.PDF Document Upload:** Upload the provided PDF document from web page (https://aclanthology.org/N19-1423.pdf) (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding).\n","\n","- **2.Chunking:** Divide the uploaded PDF document into smaller segments (chunks). This facilitates more efficient information processing by the LLM.\n","\n","- **3.ChromaDB Setup:**\n","  - Save ChromaDB to your Google Drive.\n","\n","  - Retrieve ChromaDB from your Drive to begin using it in your project.\n","\n","  - ChromaDB serves as a vector database to store embedding vectors generated from your document.\n","\n","- **4.Embedding Vectors Creation:**\n","  - Convert the chunked document into embedding vectors. You can use either GPT or Gemini embedding models for this purpose.\n","\n","  - If you choose the Gemini embedding model, set \"task_type\" to \"retrieval_document\" when converting the chunked document.\n","\n","- **5.Chatbot Development:**\n","  - Utilize the **load_qa_chain** function from the Langchain library to build the chatbot.\n","\n","  - This function will interpret user queries, retrieve relevant information from **ChromaDB**, and generate responses accordingly.\n","\n"],"metadata":{"id":"MaCz7nhxKI9R"}},{"cell_type":"markdown","source":["### Install Libraries"],"metadata":{"id":"_eoQWi-uN0dx"}},{"cell_type":"code","source":["!pip install -qU langchain-community"],"metadata":{"id":"PCbI4MuNanVu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -qU langchain-google-community"],"metadata":{"id":"qOaahY-AancA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756799028230,"user_tz":-120,"elapsed":7038,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"a61158ca-ee25-4606-f3a5-5c5fe506e3d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/99.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.6/99.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -qU langchain-openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZhYO2PlESFRB","executionInfo":{"status":"ok","timestamp":1756799073292,"user_tz":-120,"elapsed":8765,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"7ff40cf2-b9c8-470b-9f25-5403ec282d6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -qU langchain-chroma"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNnpBIfgSFT7","executionInfo":{"status":"ok","timestamp":1756799107797,"user_tz":-120,"elapsed":34501,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"ee15be39-145b-4e6b-dc3f-d076b9732551"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["!pip install -qU pypdfium2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NKZSEnuzSFXA","executionInfo":{"status":"ok","timestamp":1756799129639,"user_tz":-120,"elapsed":8993,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"45322e0d-7403-49fd-aa33-6cadf5f88661"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-6JS_vBnSFYu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Access Google Drive"],"metadata":{"id":"FuLllnCl2yfe"}},{"cell_type":"code","source":[],"metadata":{"id":"uQR06EhDapPP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Entering Your OpenAI or Google Gemini API Key."],"metadata":{"id":"0uR9bJp_0MyF"}},{"cell_type":"code","source":[],"metadata":{"id":"2jwo1SQ2asnZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","\n","os.environ['OPENAI_API_KEY']=userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"90rF1aM1astv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loading PDF Document"],"metadata":{"id":"OV9rG-0PN8p0"}},{"cell_type":"code","source":["import requests\n","from langchain_community.document_loaders import PyPDFium2Loader\n","\n","def read_doc_from_url(url):\n","    # Download the PDF file\n","    response = requests.get(url)\n","    if response.status_code != 200:\n","        raise Exception(f\"Failed to download PDF: {response.status_code}\")\n","\n","    # Save it temporarily\n","    temp_path = \"temp_downloaded.pdf\"\n","    with open(temp_path, \"wb\") as f:\n","        f.write(response.content)\n","\n","    # Load with PyPDFium2Loader\n","    loader = PyPDFium2Loader(temp_path)\n","    pdf_documents = loader.load()\n","    return pdf_documents"],"metadata":{"id":"JAjRypeKv9fn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # create a pdf reader function\n","# from langchain_community.document_loaders import PyPDFium2Loader\n","\n","# def read_doc(directory):\n","#     file_loader=PyPDFium2Loader(directory)\n","#     pdf_documents=file_loader.load()  # PyPDFium2Loader reads page by page\n","#     return pdf_documents"],"metadata":{"id":"5H6eQyYyauxP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Usage\n","url = \"https://aclanthology.org/N19-1423.pdf\"\n","pdf = read_doc_from_url(url)"],"metadata":{"id":"EOq03IViwLXl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pdf = read_doc('/content/attention is all you need.pdf')\n","\n","print(f\"=\" * 55)\n","\n","print(f\"The 'attention is all you need.pdf' file has {len(pdf)} pages.\")\n","\n","# The document consists of 16 pages"],"metadata":{"id":"n_kXJZ5Taupv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756814505853,"user_tz":-120,"elapsed":26,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"a614172b-a239-4a94-92db-06f6f1e7bae3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=======================================================\n","The 'attention is all you need.pdf' file has 16 pages.\n"]}]},{"cell_type":"code","source":["pdf[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLdZHj1sSjL-","executionInfo":{"status":"ok","timestamp":1756805376473,"user_tz":-120,"elapsed":28,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"fe135bb9-39aa-4289-864d-d0abbea4d256"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-29T17:36:03+00:00', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'subject': 'N19-1 2019', 'keywords': '', 'moddate': '2019-04-29T17:36:03+00:00', 'source': 'temp_downloaded.pdf', 'total_pages': 16, 'page': 1}, page_content='4172\\nword based only on its context. Unlike left-to\\x02right language model pre-training, the MLM ob\\x02jective enables the representation to fuse the left\\nand the right context, which allows us to pre\\x02train a deep bidirectional Transformer. In addi\\x02tion to the masked language model, we also use\\na “next sentence prediction” task that jointly pre\\x02trains text-pair representations. The contributions\\nof our paper are as follows:\\n• We demonstrate the importance of bidirectional\\npre-training for language representations. Un\\x02like Radford et al. (2018), which uses unidirec\\x02tional language models for pre-training, BERT\\nuses masked language models to enable pre\\x02trained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe need for many heavily-engineered task\\x02specific architectures. BERT is the first fine\\x02tuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper\\x02forming many task-specific architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod\\x02els are available at https://github.com/\\ngoogle-research/bert.\\n2 Related Work\\nThere is a long history of pre-training general lan\\x02guage representations, and we briefly review the\\nmost widely-used approaches in this section.\\n2.1 Unsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods. Pre-trained word embeddings\\nare an integral part of modern NLP systems, of\\x02fering significant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre\\x02train word embedding vectors, left-to-right lan\\x02guage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis\\x02criminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed\\x02dings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen\\x02tence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto\\x02encoder derived objectives (Hill et al., 2016).\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re\\x02search along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep\\x02resentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-specific architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques\\x02tion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre\\x02dict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod\\x02els.\\n2.2 Unsupervised Fine-tuning Approaches\\nAs with the feature-based approaches, the first\\nworks in this direction only pre-trained word em\\x02bedding parameters from unlabeled text (Col\\x02lobert and Weston, 2008).\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\nfine-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre\\x02viously state-of-the-art results on many sentence\\x02level tasks from the GLUE benchmark (Wang\\net al., 2018a). Left-to-right language model-\\n')"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":[],"metadata":{"id":"78dT-kxVSjOo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9AMrdjBNSjRc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Document Splitter"],"metadata":{"id":"WLQ1j_JrOF57"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n","\n","\n","def chunk_data(docs, chunk_size=1000, chunk_overlap=200):\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,        # each chunk will be a maximum of 1000 characters long\n","                                                   chunk_overlap=chunk_overlap)  # each chunk will share a 200-character overlap with the previous chunk to maintain context\n","    pdf = text_splitter.split_documents(docs)\n","    return pdf\n","# This code splits documents into chunks using the RecursiveCharacterTextSplitter class from the langchain library."],"metadata":{"id":"HHQlclU9awwa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pdf_doc = chunk_data(docs=pdf)\n","\n","len(pdf_doc)\n","\n","# divided into 53 pieces"],"metadata":{"id":"NaQV6XRwawpf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756814523109,"user_tz":-120,"elapsed":68,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"b7109b8d-806d-461a-e0b7-cd1c7df2ea35"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["83"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["pdf_doc[25:27]"],"metadata":{"id":"tGOqA1C0S6iM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6oSmcdpNS6ki"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1. Creating A Embedding Model\n"],"metadata":{"id":"4ENim_5MOT9O"}},{"cell_type":"code","source":["from langchain_openai import OpenAIEmbeddings\n","\n","embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\",\n","                              dimensions=3072)  # dimensions=256, 1024, 3072\n","embeddings"],"metadata":{"id":"96nLFF1ja0k_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756814530867,"user_tz":-120,"elapsed":2551,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"b2081d8c-868e-4d15-9a9b-9bf5ed632401"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7e9853ec30b0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7e9853459970>, model='text-embedding-3-large', dimensions=3072, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["text = \"This is a test document.\""],"metadata":{"id":"VqZ7XBwoa0ee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc_result = embeddings.embed_documents([text])\n","\n","# We convert the document into embedding vector"],"metadata":{"id":"Ky_XttEqxdhK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc_result[0][:2]\n","# First 5 elements of 3072 dimensional embedding vector"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"leNC9O8hxdka","executionInfo":{"status":"ok","timestamp":1756814627294,"user_tz":-120,"elapsed":9,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"b1c0e85b-c32c-4f36-82fc-0de1756b076e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-0.014371714554727077, -0.027211420238018036]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["len(doc_result[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9TI-sk85xh7Q","executionInfo":{"status":"ok","timestamp":1756814633482,"user_tz":-120,"elapsed":11,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"ccd4ed6f-bd2b-4d6d-dc1c-d93480c4b673"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3072"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":[],"metadata":{"id":"M2TA3ztyA3u_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Convert the Each Chunk of The Split Document to Embedding Vectors\n","### 3. Storing of The Embedding Vectors to Vectorstore\n","### 4. Save the Vectorstore to Your Drive\n"],"metadata":{"id":"VygPrgIxBM2w"}},{"cell_type":"code","source":["from langchain_chroma import Chroma\n","\n","index = Chroma.from_documents(documents=pdf_doc,\n","                              embedding=embeddings,\n","                              persist_directory=\"./vectorstore\")  # persist_directory, saves in the directory\n","\n","# Create a retriever object from our Chroma vector store (index).\n","retriever = index.as_retriever()  # for using agents #index.as_retriever(search_kwargs={\"k\": 4})"],"metadata":{"id":"sxk1eJZABRX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever = index.as_retriever(search_kwargs={\"k\": 4})"],"metadata":{"id":"sQvqXzrABRb6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever.invoke(\"Can you explain me BERT relation with TPU?\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9hyVblWIBRfX","executionInfo":{"status":"ok","timestamp":1756814824902,"user_tz":-120,"elapsed":907,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"ddc04238-bd36-43df-daee-c9f0b964626e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(id='79bc2178-bfc3-4f61-aec2-413b8a3f4c00', metadata={'source': 'temp_downloaded.pdf', 'creator': 'LaTeX with hyperref package', 'subject': 'N19-1 2019', 'producer': 'pdfTeX-1.40.18', 'page': 2, 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'total_pages': 16, 'keywords': '', 'moddate': '2019-04-29T17:36:03+00:00', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'creationdate': '2019-04-29T17:36:03+00:00'}, page_content='framework: pre-training and fine-tuning. Dur\\x02ing pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For fine\\x02tuning, the BERT model is first initialized with\\nthe pre-trained parameters, and all of the param\\x02eters are fine-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep\\x02arate fine-tuned models, even though they are ini\\x02tialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its unified ar\\x02chitecture across different tasks. There is mini\\x02mal difference between the pre-trained architec\\x02ture and the final downstream architecture.\\nModel Architecture BERT’s model architec\\x02ture is a multi-layer bidirectional Transformer en\\x02coder based on the original implementation de\\x02scribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use'),\n"," Document(id='fd4dd784-bd02-40ad-bfc6-687792432e0f', metadata={'subject': 'N19-1 2019', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'creationdate': '2019-04-29T17:36:03+00:00', 'page': 2, 'keywords': '', 'moddate': '2019-04-29T17:36:03+00:00', 'source': 'temp_downloaded.pdf', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'producer': 'pdfTeX-1.40.18', 'total_pages': 16, 'creator': 'LaTeX with hyperref package'}, page_content='the tensor2tensor library.1 Because the use\\nof Transformers has become common and our im\\x02plementation is almost identical to the original,\\nwe will omit an exhaustive background descrip\\x02tion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.\\n3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param\\x02eters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans\\x02former uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1\\nhttps://github.com/tensorflow/tensor2tensor\\n2'),\n"," Document(id='fad4ae02-b6dd-4c40-9b4f-3cf0be21fea1', metadata={'keywords': '', 'page': 0, 'producer': 'pdfTeX-1.40.18', 'subject': 'N19-1 2019', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-29T17:36:03+00:00', 'source': 'temp_downloaded.pdf', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'total_pages': 16, 'moddate': '2019-04-29T17:36:03+00:00', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'}, page_content='to create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan'),\n"," Document(id='5e9b0531-fec8-4504-a4df-49c04cb18120', metadata={'subject': 'N19-1 2019', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-29T17:36:03+00:00', 'producer': 'pdfTeX-1.40.18', 'keywords': '', 'source': 'temp_downloaded.pdf', 'moddate': '2019-04-29T17:36:03+00:00', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'page': 12, 'total_pages': 16, 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'}, page_content='rate warmup over the first 10,000 steps, and linear\\ndecay of the learning rate. We use a dropout prob\\x02ability of 0.1 on all layers. We use a gelu acti\\x02vation (Hendrycks and Gimpel, 2016) rather than\\nthe standard relu, following OpenAI GPT. The\\ntraining loss is the sum of the mean masked LM\\nlikelihood and the mean next sentence prediction\\nlikelihood.\\nTraining of BERTBASE was performed on 4\\nCloud TPUs in Pod configuration (16 TPU chips\\ntotal).13 Training of BERTLARGE was performed\\non 16 Cloud TPUs (64 TPU chips total). Each pre\\x02training took 4 days to complete.\\nLonger sequences are disproportionately expen\\x02sive because attention is quadratic to the sequence\\nlength. To speed up pretraing in our experiments,\\nwe pre-train the model with sequence length of\\n128 for 90% of the steps. Then, we train the rest\\n10% of the steps of sequence of 512 to learn the\\npositional embeddings.\\nA.3 Fine-tuning Procedure\\nFor fine-tuning, most model hyperparameters are')]"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":[],"metadata":{"id":"U5oPG5POBK5y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Vectorstore(index) From Your Drive"],"metadata":{"id":"y2tMqUthPchD"}},{"cell_type":"code","source":["# Initiates the Chroma class, which serves as our vector database.\n","\n","loaded_index = Chroma(persist_directory=\"./vectorstore\",\n","                      embedding_function=embeddings)"],"metadata":{"id":"5pjKXmO6a3En"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["load_retriver = loaded_index.as_retriever(search_kwargs={\"k\": 4})"],"metadata":{"id":"AXQwX35ha282"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Retrival the First 5 Chunks That Are Most Similar to The User Query from The Document"],"metadata":{"id":"pKA0PgNJQOmj"}},{"cell_type":"code","source":["def retrieve_query(query, k=4):\n","    retriever = index.as_retriever(search_kwargs={\"k\": k})  # loaded_index\n","    return retriever.invoke(query)\n"],"metadata":{"id":"KRH-FWEua5Fn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["our_query = \"Can you explain me BERT relation with TPU?\"\n","\n","doc_search = retrieve_query(our_query, k=4)  # first two most similar texts are returned\n","doc_search"],"metadata":{"id":"80TWdFk6a4-3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756814869945,"user_tz":-120,"elapsed":1001,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"b6b2cc80-dcd9-427d-8a33-cc573cbb1dcf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(id='79bc2178-bfc3-4f61-aec2-413b8a3f4c00', metadata={'total_pages': 16, 'source': 'temp_downloaded.pdf', 'subject': 'N19-1 2019', 'producer': 'pdfTeX-1.40.18', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'keywords': '', 'moddate': '2019-04-29T17:36:03+00:00', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'creator': 'LaTeX with hyperref package', 'page': 2, 'creationdate': '2019-04-29T17:36:03+00:00'}, page_content='framework: pre-training and fine-tuning. Dur\\x02ing pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For fine\\x02tuning, the BERT model is first initialized with\\nthe pre-trained parameters, and all of the param\\x02eters are fine-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep\\x02arate fine-tuned models, even though they are ini\\x02tialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its unified ar\\x02chitecture across different tasks. There is mini\\x02mal difference between the pre-trained architec\\x02ture and the final downstream architecture.\\nModel Architecture BERT’s model architec\\x02ture is a multi-layer bidirectional Transformer en\\x02coder based on the original implementation de\\x02scribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use'),\n"," Document(id='fd4dd784-bd02-40ad-bfc6-687792432e0f', metadata={'creator': 'LaTeX with hyperref package', 'moddate': '2019-04-29T17:36:03+00:00', 'source': 'temp_downloaded.pdf', 'keywords': '', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'total_pages': 16, 'page': 2, 'producer': 'pdfTeX-1.40.18', 'subject': 'N19-1 2019', 'creationdate': '2019-04-29T17:36:03+00:00', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'}, page_content='the tensor2tensor library.1 Because the use\\nof Transformers has become common and our im\\x02plementation is almost identical to the original,\\nwe will omit an exhaustive background descrip\\x02tion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.\\n3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param\\x02eters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans\\x02former uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1\\nhttps://github.com/tensorflow/tensor2tensor\\n2'),\n"," Document(id='fad4ae02-b6dd-4c40-9b4f-3cf0be21fea1', metadata={'subject': 'N19-1 2019', 'producer': 'pdfTeX-1.40.18', 'source': 'temp_downloaded.pdf', 'keywords': '', 'total_pages': 16, 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'creator': 'LaTeX with hyperref package', 'moddate': '2019-04-29T17:36:03+00:00', 'page': 0, 'creationdate': '2019-04-29T17:36:03+00:00'}, page_content='to create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan'),\n"," Document(id='5e9b0531-fec8-4504-a4df-49c04cb18120', metadata={'keywords': '', 'moddate': '2019-04-29T17:36:03+00:00', 'creationdate': '2019-04-29T17:36:03+00:00', 'total_pages': 16, 'page': 12, 'source': 'temp_downloaded.pdf', 'author': 'Jacob Devlin ; Ming-Wei Chang ; Kenton Lee ; Kristina Toutanova', 'creator': 'LaTeX with hyperref package', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'subject': 'N19-1 2019', 'producer': 'pdfTeX-1.40.18'}, page_content='rate warmup over the first 10,000 steps, and linear\\ndecay of the learning rate. We use a dropout prob\\x02ability of 0.1 on all layers. We use a gelu acti\\x02vation (Hendrycks and Gimpel, 2016) rather than\\nthe standard relu, following OpenAI GPT. The\\ntraining loss is the sum of the mean masked LM\\nlikelihood and the mean next sentence prediction\\nlikelihood.\\nTraining of BERTBASE was performed on 4\\nCloud TPUs in Pod configuration (16 TPU chips\\ntotal).13 Training of BERTLARGE was performed\\non 16 Cloud TPUs (64 TPU chips total). Each pre\\x02training took 4 days to complete.\\nLonger sequences are disproportionately expen\\x02sive because attention is quadratic to the sequence\\nlength. To speed up pretraing in our experiments,\\nwe pre-train the model with sequence length of\\n128 for 90% of the steps. Then, we train the rest\\n10% of the steps of sequence of 512 to learn the\\npositional embeddings.\\nA.3 Fine-tuning Procedure\\nFor fine-tuning, most model hyperparameters are')]"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["# from IPython.display import Markdown\n","\n","#Markdown(doc_search.page_content)"],"metadata":{"id":"FOJN80PsVDVy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generating an Answer Based on The Similar Chunks"],"metadata":{"id":"-G8R4V7BROkz"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate, ChatPromptTemplate\n","\n","template=\"\"\"Use the following pieces of context to answer the user's question of \"{question}\".\n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","----------------\n","\"{context}\" \"\"\"\n","\n","prompt_template = PromptTemplate(\n","    input_variables=['question','context'],\n","    template=template\n","    )"],"metadata":{"id":"XNDU0jcma7HB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["our_query = \"Can you explain me BERT relation with TPU?\""],"metadata":{"id":"aUnTIxSoa6_y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","\n","llm=ChatOpenAI(model_name=\"gpt-4o-mini\",\n","               temperature=0,\n","               top_p=1)\n","\n","chain = prompt_template | llm | StrOutputParser()\n","\n","output= chain.invoke({\"question\":our_query, \"context\":doc_search})  # first four most similar texts are returned\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"qYvXlv_ZSjRq","executionInfo":{"status":"ok","timestamp":1756818174065,"user_tz":-120,"elapsed":4336,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"6cfe697b-c599-453e-c39f-a8f7e80b41f0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'BERT (Bidirectional Encoder Representations from Transformers) is a model architecture that utilizes a multi-layer bidirectional Transformer encoder. The training of BERTBASE and BERTLARGE models was performed on Cloud TPUs (Tensor Processing Units). Specifically, BERTBASE was trained on 4 Cloud TPUs (16 TPU chips total), while BERTLARGE was trained on 16 Cloud TPUs (64 TPU chips total). The use of TPUs significantly accelerated the training process, allowing the researchers to complete the pre-training in just 4 days. \\n\\nIn summary, the relationship between BERT and TPU is that TPUs were used as the hardware for training the BERT models, enabling efficient processing and faster training times.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["from IPython.display import Markdown\n","\n","Markdown(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"J5r5bzI6SjUs","executionInfo":{"status":"ok","timestamp":1756818308183,"user_tz":-120,"elapsed":15,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"053bd5a9-8825-4ce1-e68a-c258edd61df9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"BERT (Bidirectional Encoder Representations from Transformers) is a model architecture that utilizes a multi-layer bidirectional Transformer encoder. The training of BERTBASE and BERTLARGE models was performed on Cloud TPUs (Tensor Processing Units). Specifically, BERTBASE was trained on 4 Cloud TPUs (16 TPU chips total), while BERTLARGE was trained on 16 Cloud TPUs (64 TPU chips total). The use of TPUs significantly accelerated the training process, allowing the researchers to complete the pre-training in just 4 days. \n\nIn summary, the relationship between BERT and TPU is that TPUs were used as the hardware for training the BERT models, enabling efficient processing and faster training times."},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["#This function represents the complete workflow from receiving a user's question to generating a final, coherent answer.\n","def get_answers(query, k=4):\n","    from langchain_openai import ChatOpenAI\n","    from langchain_core.output_parsers import StrOutputParser\n","    from langchain.prompts import PromptTemplate\n","    from IPython.display import Markdown\n","\n","    doc_search=retrieve_query(query, k=k)  # most similar texts are returned\n","\n","\n","    template=\"\"\"Use the following pieces of context to answer the user's question of {question}.\n","    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","    ----------------\n","    {context}\"\"\"\n","\n","    prompt_template = PromptTemplate(input_variables =['question', 'context'],\n","                                     template=template)\n","\n","\n","    llm=ChatOpenAI(model_name=\"gpt-4o-mini\",\n","                   temperature=0,\n","                   top_p=1)\n","\n","    chain = prompt_template | llm | StrOutputParser()\n","\n","    output= chain.invoke({\"question\":query, \"context\":doc_search}) # first four most similar texts are returned\n","    return Markdown(output)"],"metadata":{"id":"sxO7VVT-SjYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["our_query = \"Can you explain me BERT relation with TPU?\"\n","\n","answer = get_answers(our_query, k=5)\n","answer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":116},"id":"OTA2EEInSxSh","executionInfo":{"status":"ok","timestamp":1756818349158,"user_tz":-120,"elapsed":4700,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"e05ad83a-ac42-4b69-b3a9-71e58b7e475f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"BERT (Bidirectional Encoder Representations from Transformers) is a model architecture designed for natural language processing tasks, and it was trained using Cloud TPUs (Tensor Processing Units). Specifically, the training of BERTBASE was performed on 4 Cloud TPUs in a Pod configuration, totaling 16 TPU chips, while BERTLARGE was trained on 16 Cloud TPUs, amounting to 64 TPU chips. The use of TPUs allowed for efficient training of the model, which took 4 days to complete for each version. The document does not provide further details on the specific relationship between BERT and TPUs beyond their use in the training process."},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["our_query = \"Can you explain me BERT relation with GPU?\"\n","\n","answer = get_answers(our_query, k=8)\n","answer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"DneLvE-9SxVx","executionInfo":{"status":"ok","timestamp":1756818369391,"user_tz":-120,"elapsed":4384,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"b64edd9a-22db-45b0-bcb2-ec4dc4b8646f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"I don't know."},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["### Pipeline For RAG"],"metadata":{"id":"sy4fmzsWLayT"}},{"cell_type":"code","source":["# #This is a crucial step if you plan on using agents that rely on external search tools.\n","# #Here it is not used.\n","\n","# import os\n","# from google.colab import userdata\n","\n","# os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')"],"metadata":{"id":"HAPVSZnljltH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #The following code snippet is setting up an external search tool for a LangChain agent.\n","# from langchain_community.tools.tavily_search import TavilySearchResults\n","\n","# search_tool = TavilySearchResults(max_results=1,\n","#                                   search_depth=\"basic\", #advanced\n","#                                   include_answer=True,\n","#                                   include_raw_content=True\n","#                                   )"],"metadata":{"id":"0qfzBxwyjlwc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_chroma import Chroma\n","\n","# index=Chroma.from_documents(documents=pdf_doc,\n","#                            embedding=embeddings,\n","#                            persist_directory=\"./vectorstore\") # persist_directory, saves in the directory\n","\n","retriever_new = index.as_retriever(search_kwargs={\"k\": 5})"],"metadata":{"id":"b10ZKtewjl0D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.tools.retriever import create_retriever_tool\n","\n","retriever_tool = create_retriever_tool(retriever=retriever_new,  # This specifies the retriever object that the tool will use.\n","                                       # This gives the tool a unique and descriptive name.\n","                                       name=\"BERT_doc_inhalt_search\",\n","                                       # This description tells the agent when it should use this specific tool.\n","                                       description=\"Search for information about BERT . For any questions about BERT. \\\n","                                                    For any question about article of 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\\\n","                                                    you must use this tool!.If you don't know the answer, just say that you don't know, don't try to make up an answer.\")"],"metadata":{"id":"y_YIEr7bkAgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever_tool.name"],"metadata":{"id":"_aSt7YgIa9jo","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1756819310342,"user_tz":-120,"elapsed":8,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"b315f85a-faa7-445a-9b82-6f96b8444b22"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'BERT_doc_inhalt_search'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["!pip install -qU langgraph"],"metadata":{"id":"2PzfTncDa9dO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_input = \"Can you explain me BERT relation with TPU?\""],"metadata":{"id":"-gdaUx-sDKXR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.messages import SystemMessage, HumanMessage\n","from langgraph.prebuilt import create_react_agent\n","\n","# Instantiate the gpt-4o-mini language model, which will act as the \"brain\" of our agent.\n","llm = ChatOpenAI(temperature=0.0,\n","                 model=\"gpt-4o-mini\",\n","                 top_p=1)\n","\n","# Define a system-level prompt that guides the agent's behavior.\n","prompt = \"Make sure to use the BERT_doc_inhalt_search tool for questions, \\\n","and If you don't know the answer, just say that you don't know, \\\n","don't try to make up an answer.\"# the tavily_search_results_json tool for questions you don't know about.\"\n","\n","# Create a list of the tools that the agent has access to.\n","tools = [retriever_tool]  # [retriever_tool, search_tool]\n","\n","# Bind the list of tools to the LLM, making the LLM aware of what tools it can use and how to call them.\n","model_with_tools = llm.bind_tools(tools)\n","\n","# Create the actual agent executor.\n","agent_executor = create_react_agent(model_with_tools, tools)  # system mesajı kullanıyorsan state_modifier tanımlamana gerek yok\n","\n","# This is where the agent is actually invoked with a user's question (user_input).\n","# The agent will receive the question, decide whether to use one of its tools or generate a direct response, and then return the result.\n","response = agent_executor.invoke({\"messages\": [HumanMessage(content=user_input)]})  # SystemMessage(content=prompt),\n","\n","response[\"messages\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LskCqCoCDKaA","executionInfo":{"status":"ok","timestamp":1756819331320,"user_tz":-120,"elapsed":7964,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"ecad17d6-f9af-4f39-dc98-457941c3c89c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[HumanMessage(content='Can you explain me BERT relation with TPU?', additional_kwargs={}, response_metadata={}, id='8c4d2e6a-b46d-405c-88ad-ca2d86152c17'),\n"," AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_o2Qg78hzhFaVi2qVsBNcVNCI', 'function': {'arguments': '{\"query\":\"BERT TPU\"}', 'name': 'BERT_doc_inhalt_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 125, 'total_tokens': 145, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CBLFUVKeCvX0rCvbsalZlKUitbsQN', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c337885b-1e7d-417c-b6b7-a48eca97c5eb-0', tool_calls=[{'name': 'BERT_doc_inhalt_search', 'args': {'query': 'BERT TPU'}, 'id': 'call_o2Qg78hzhFaVi2qVsBNcVNCI', 'type': 'tool_call'}], usage_metadata={'input_tokens': 125, 'output_tokens': 20, 'total_tokens': 145, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n"," ToolMessage(content='to create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\n\\n4173\\nBERT BERT\\nE[CLS] E1\\n E[SEP] ... EN\\nE1’ ... EM’\\nC T1 T[SEP] ... TN\\nT1’ ... TM’\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1\\n E[SEP] ... EN\\nE1’ ... EM’\\nC T1 T[SEP] ... TN\\nT1’ ... TM’\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\nMasked Sentence A Masked Sentence B\\nPre-training Fine-Tuning\\nNSP Mask LM Mask LM\\nUnlabeled Sentence A and B Pair \\nSQuAD\\nQuestion Answer Pair\\nMNLI NER\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\\x02tions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n\\napproach.\\nThe most comparable existing pre-training\\nmethod to BERT is OpenAI GPT, which trains a\\nleft-to-right Transformer LM on a large text cor\\x02pus. In fact, many of the design decisions in BERT\\nwere intentionally made to make it as close to\\nGPT as possible so that the two methods could be\\nminimally compared. The core argument of this\\nwork is that the bi-directionality and the two pre\\x02training tasks presented in Section 3.1 account for\\nthe majority of the empirical improvements, but\\nwe do note that there are several other differences\\nbetween how BERT and GPT were trained:\\n• GPT is trained on the BooksCorpus (800M\\nwords); BERT is trained on the BooksCor\\x02pus (800M words) and Wikipedia (2,500M\\nwords).\\n• GPT uses a sentence separator ([SEP]) and\\nclassifier token ([CLS]) which are only in\\x02troduced at fine-tuning time; BERT learns\\n[SEP], [CLS] and sentence A/B embed\\x02dings during pre-training.\\n• GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for\\n\\nrate warmup over the first 10,000 steps, and linear\\ndecay of the learning rate. We use a dropout prob\\x02ability of 0.1 on all layers. We use a gelu acti\\x02vation (Hendrycks and Gimpel, 2016) rather than\\nthe standard relu, following OpenAI GPT. The\\ntraining loss is the sum of the mean masked LM\\nlikelihood and the mean next sentence prediction\\nlikelihood.\\nTraining of BERTBASE was performed on 4\\nCloud TPUs in Pod configuration (16 TPU chips\\ntotal).13 Training of BERTLARGE was performed\\non 16 Cloud TPUs (64 TPU chips total). Each pre\\x02training took 4 days to complete.\\nLonger sequences are disproportionately expen\\x02sive because attention is quadratic to the sequence\\nlength. To speed up pretraing in our experiments,\\nwe pre-train the model with sequence length of\\n128 for 90% of the steps. Then, we train the rest\\n10% of the steps of sequence of 512 to learn the\\npositional embeddings.\\nA.3 Fine-tuning Procedure\\nFor fine-tuning, most model hyperparameters are\\n\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classification, such as en\\x02tailment or sentiment analysis.\\nCompared to pre-training, fine-tuning is rela\\x02tively inexpensive. All of the results in the pa\\x02per can be replicated in at most 1 hour on a sin\\x02gle Cloud TPU, or a few hours on a GPU, starting\\nfrom the exact same pre-trained model.7 We de\\x02scribe the task-specific details in the correspond\\x02ing subsections of Section 4. More details can be\\nfound in Appendix A.5.\\n4 Experiments\\nIn this section, we present BERT fine-tuning re\\x02sults on 11 NLP tasks.\\n4.1 GLUE\\nThe General Language Understanding Evaluation\\n(GLUE) benchmark (Wang et al., 2018a) is a col\\x02lection of diverse natural language understanding\\ntasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo fine-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R\\nH corresponding to the first', name='BERT_doc_inhalt_search', id='9055726e-0439-431b-9d6e-4a8ea5014d40', tool_call_id='call_o2Qg78hzhFaVi2qVsBNcVNCI'),\n"," AIMessage(content=\"BERT (Bidirectional Encoder Representations from Transformers) is a powerful model for natural language processing that was trained using Cloud TPUs (Tensor Processing Units). Specifically, the training of BERTBASE was performed on 4 Cloud TPUs in a Pod configuration, which consists of 16 TPU chips in total. The larger model, BERTLARGE, was trained on 16 Cloud TPUs, totaling 64 TPU chips. Each pre-training session took about 4 days to complete.\\n\\nThe use of TPUs is significant because they are designed to accelerate machine learning workloads, particularly those involving large-scale neural networks like BERT. The training process involves handling large amounts of data and performing complex computations, which TPUs are optimized for, allowing for faster training times compared to traditional GPUs.\\n\\nIn summary, BERT's relationship with TPUs lies in the efficient training of the model on these specialized hardware units, which enables the handling of extensive datasets and complex model architectures effectively.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 195, 'prompt_tokens': 1458, 'total_tokens': 1653, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-CBLFXu91wta0igx6LMIS1RGbs6eVt', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--aed93603-ac23-40f5-bdf6-9844a5736824-0', usage_metadata={'input_tokens': 1458, 'output_tokens': 195, 'total_tokens': 1653, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["for chunk in agent_executor.stream(\n","    {\"messages\": [(\"human\", user_input)]}\n","):\n","    print(chunk)\n","    print(\"----\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_GPn4X1qDKcy","executionInfo":{"status":"ok","timestamp":1756819347284,"user_tz":-120,"elapsed":9796,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"0ae70556-4ee2-416c-ceab-f4e2dd277604"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_fyHR77jcs78QGrNZUFgVyuXc', 'function': {'arguments': '{\"query\":\"BERT TPU\"}', 'name': 'BERT_doc_inhalt_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 125, 'total_tokens': 145, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-CBLFhWgU6LYgqvWmdGwgeltbZVhqV', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--130c31fd-f2d7-4562-93fa-e0b755546a41-0', tool_calls=[{'name': 'BERT_doc_inhalt_search', 'args': {'query': 'BERT TPU'}, 'id': 'call_fyHR77jcs78QGrNZUFgVyuXc', 'type': 'tool_call'}], usage_metadata={'input_tokens': 125, 'output_tokens': 20, 'total_tokens': 145, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n","----\n","{'tools': {'messages': [ToolMessage(content='to create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task\\x02specific architecture modifications.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re\\x02sults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer\\x02ing Test F1 to 93.2 (1.5 point absolute im\\x02provement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\n\\n4173\\nBERT BERT\\nE[CLS] E1\\n E[SEP] ... EN\\nE1’ ... EM’\\nC T1 T[SEP] ... TN\\nT1’ ... TM’\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1\\n E[SEP] ... EN\\nE1’ ... EM’\\nC T1 T[SEP] ... TN\\nT1’ ... TM’\\n[CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM\\nMasked Sentence A Masked Sentence B\\nPre-training Fine-Tuning\\nNSP Mask LM Mask LM\\nUnlabeled Sentence A and B Pair \\nSQuAD\\nQuestion Answer Pair\\nMNLI NER\\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec\\x02tures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques\\x02tions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n\\napproach.\\nThe most comparable existing pre-training\\nmethod to BERT is OpenAI GPT, which trains a\\nleft-to-right Transformer LM on a large text cor\\x02pus. In fact, many of the design decisions in BERT\\nwere intentionally made to make it as close to\\nGPT as possible so that the two methods could be\\nminimally compared. The core argument of this\\nwork is that the bi-directionality and the two pre\\x02training tasks presented in Section 3.1 account for\\nthe majority of the empirical improvements, but\\nwe do note that there are several other differences\\nbetween how BERT and GPT were trained:\\n• GPT is trained on the BooksCorpus (800M\\nwords); BERT is trained on the BooksCor\\x02pus (800M words) and Wikipedia (2,500M\\nwords).\\n• GPT uses a sentence separator ([SEP]) and\\nclassifier token ([CLS]) which are only in\\x02troduced at fine-tuning time; BERT learns\\n[SEP], [CLS] and sentence A/B embed\\x02dings during pre-training.\\n• GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for\\n\\nrate warmup over the first 10,000 steps, and linear\\ndecay of the learning rate. We use a dropout prob\\x02ability of 0.1 on all layers. We use a gelu acti\\x02vation (Hendrycks and Gimpel, 2016) rather than\\nthe standard relu, following OpenAI GPT. The\\ntraining loss is the sum of the mean masked LM\\nlikelihood and the mean next sentence prediction\\nlikelihood.\\nTraining of BERTBASE was performed on 4\\nCloud TPUs in Pod configuration (16 TPU chips\\ntotal).13 Training of BERTLARGE was performed\\non 16 Cloud TPUs (64 TPU chips total). Each pre\\x02training took 4 days to complete.\\nLonger sequences are disproportionately expen\\x02sive because attention is quadratic to the sequence\\nlength. To speed up pretraing in our experiments,\\nwe pre-train the model with sequence length of\\n128 for 90% of the steps. Then, we train the rest\\n10% of the steps of sequence of 512 to learn the\\npositional embeddings.\\nA.3 Fine-tuning Procedure\\nFor fine-tuning, most model hyperparameters are\\n\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classification, such as en\\x02tailment or sentiment analysis.\\nCompared to pre-training, fine-tuning is rela\\x02tively inexpensive. All of the results in the pa\\x02per can be replicated in at most 1 hour on a sin\\x02gle Cloud TPU, or a few hours on a GPU, starting\\nfrom the exact same pre-trained model.7 We de\\x02scribe the task-specific details in the correspond\\x02ing subsections of Section 4. More details can be\\nfound in Appendix A.5.\\n4 Experiments\\nIn this section, we present BERT fine-tuning re\\x02sults on 11 NLP tasks.\\n4.1 GLUE\\nThe General Language Understanding Evaluation\\n(GLUE) benchmark (Wang et al., 2018a) is a col\\x02lection of diverse natural language understanding\\ntasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo fine-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)\\nas described in Section 3, and use the final hid\\x02den vector C ∈ R\\nH corresponding to the first', name='BERT_doc_inhalt_search', id='29083c89-c7d6-4d3d-9435-1781d5c87f2d', tool_call_id='call_fyHR77jcs78QGrNZUFgVyuXc')]}}\n","----\n","{'agent': {'messages': [AIMessage(content='BERT (Bidirectional Encoder Representations from Transformers) is a powerful model for natural language processing that was trained using Cloud TPUs (Tensor Processing Units). Specifically, the training of BERTBASE was performed on 4 Cloud TPUs in a Pod configuration, which consists of 16 TPU chips in total. For the larger BERTLARGE model, training was conducted on 16 Cloud TPUs, totaling 64 TPU chips. Each pre-training session took about 4 days to complete.\\n\\nThe use of TPUs is significant because they are designed to accelerate machine learning workloads, particularly those involving large-scale neural networks like BERT. The training process involves handling long sequences, which can be computationally expensive due to the quadratic complexity of the attention mechanism relative to sequence length. To optimize training time, BERT was pre-trained with shorter sequences for the majority of the steps before transitioning to longer sequences to learn positional embeddings.\\n\\nIn summary, TPUs play a crucial role in efficiently training BERT by providing the necessary computational power to handle its complex architecture and large datasets.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 214, 'prompt_tokens': 1458, 'total_tokens': 1672, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1408}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-CBLFlcIRQFuLcYiod9OdTJa5EMWhZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ae93a041-f728-4589-927e-5905a37aa3e7-0', usage_metadata={'input_tokens': 1458, 'output_tokens': 214, 'total_tokens': 1672, 'input_token_details': {'audio': 0, 'cache_read': 1408}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n","----\n"]}]},{"cell_type":"code","source":["response[\"messages\"][-1].content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"vPmsnpWfDKfk","executionInfo":{"status":"ok","timestamp":1756819349286,"user_tz":-120,"elapsed":24,"user":{"displayName":"Ayten Cerman","userId":"17379170840993528040"}},"outputId":"5fe79390-88d3-4c34-ee34-6a9c7b1a54a8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"BERT (Bidirectional Encoder Representations from Transformers) is a powerful model for natural language processing that was trained using Cloud TPUs (Tensor Processing Units). Specifically, the training of BERTBASE was performed on 4 Cloud TPUs in a Pod configuration, which consists of 16 TPU chips in total. The larger model, BERTLARGE, was trained on 16 Cloud TPUs, totaling 64 TPU chips. Each pre-training session took about 4 days to complete.\\n\\nThe use of TPUs is significant because they are designed to accelerate machine learning workloads, particularly those involving large-scale neural networks like BERT. The training process involves handling large amounts of data and performing complex computations, which TPUs are optimized for, allowing for faster training times compared to traditional GPUs.\\n\\nIn summary, BERT's relationship with TPUs lies in the efficient training of the model on these specialized hardware units, which enables the handling of extensive datasets and complex model architectures effectively.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":[],"metadata":{"id":"GHFFW_o8DKiQ"},"execution_count":null,"outputs":[]}]}